{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of Attention in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Basics: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax(X): \n",
    "    # Computing element wise exponential value\n",
    "    X_exp = np.exp(X)\n",
    "    \n",
    "    # Computing sum of these values\n",
    "    partition = np.sum(X_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    # Returing the softmax output.\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096],\n",
       "       [0.09003057, 0.24472847, 0.66524096]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical issues: Numeric stability\n",
    "- https://cs231n.github.io/linear-classify/#softmax\n",
    "- https://stackoverflow.com/questions/50170011/adapting-pytorch-softmax-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/yyg6mz_j3vvccj0txlxd04_c0000gn/T/ipykernel_33083/1701104007.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  X_exp = np.exp(X)\n",
      "/var/folders/4w/yyg6mz_j3vvccj0txlxd04_c0000gn/T/ipykernel_33083/1701104007.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  return X_exp / partition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan],\n",
       "       [0.09003057, 0.24472847, 0.66524096]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[10000000000,200000000000,3000000000],[4,5,6]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10000000000, 200000000000,   3000000000],\n",
       "       [           4,            5,            6]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcome the numerical stability issue, overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        ]\n",
      " [1.50321472]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        ],\n",
       "       [0.09003057, 0.24472847, 0.66524096]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_np(x):\n",
    "    # get the max value for each row\n",
    "    maxes = np.max(x, axis=-1, keepdims=True)\n",
    "    # Computing element wise exponential value\n",
    "    x_exp = np.exp(x-maxes)\n",
    "    x_exp_sum = np.sum(x_exp, axis=-1, keepdims=True)\n",
    "    print(x_exp_sum)\n",
    "    probs = x_exp/x_exp_sum\n",
    "    return probs \n",
    "\n",
    "softmax_np(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax_torch(x): # Assuming x has atleast 2 dimensions\n",
    "    maxes = torch.max(x, dim=-1, keepdim=True)[0]\n",
    "    x_exp = torch.exp(x-maxes)\n",
    "    x_exp_sum = torch.sum(x_exp, dim=-1, keepdim=True)\n",
    "    probs = x_exp/x_exp_sum\n",
    "    return probs \n",
    "\n",
    "softmax_torch(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.tensor([[10000000,2,3],[4,5,6]])\n",
    "softmax_torch(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200000000000],\n",
       "       [           6]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[200000000000],\n",
       "        [           6]]),\n",
       "indices=tensor([[1],\n",
       "        [2]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor(x), dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assume Q, K, V are two dimensional matrices for simplicity\n",
    "\n",
    "self attention: Softmax((Q*K.T)/sqrt(m)) * V\n",
    "\n",
    "- Q: Query, K: Key, V: Value\n",
    "\n",
    "- Shape of Q and K: n,q (Q and K have the same dimension)\n",
    "- Shape of V: n,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/ScaledDotProductAttention.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_1(Q, K, V):\n",
    "    # Q: [n, q];  n: length of sequence, q: dimension of embedding \n",
    "    # K: [n, k];  n: length of sequence, k: dimension of embedding \n",
    "    # V: [n, v]   n: length of sequence, v: dimension of embedding \n",
    "    # in practice, q=k=m\n",
    "    n, m = Q.shape\n",
    "    # Compute the attention weights\n",
    "    attention_weights = softmax_np(Q@K.T/np.sqrt(m)) # n,n\n",
    "    # Compute the output\n",
    "    output = attention_weights@V # [n,n]*[n,v] = [n,v]\n",
    "    return output # n,v\n",
    "\n",
    "def attention_2(Q, K, V):\n",
    "    # Q: [n, q];  n: length of sequence, q: dimension of embedding \n",
    "    # K: [n, k];  n: length of sequence, k: dimension of embedding \n",
    "    # V: [n, v]   n: length of sequence, v: dimension of embedding \n",
    "    # in practice, q=k=m\n",
    "    n, m = Q.shape\n",
    "    # calculate the dot product of Q and K\n",
    "    attention_score = np.dot(Q, K.T).astype(float) # n,n\n",
    "    print(attention_score)\n",
    "    # Scale the attention score\n",
    "    attention_score /= np.sqrt(m) # n,n\n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = softmax_np(attention_score) # n,n\n",
    "    # Use attention weights to weigh the values V\n",
    "    output = np.dot(attention_weights, V) # [n,n]*[n,v] = [n,v]\n",
    "    return output # n,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09932072]\n",
      " [1.01757194]]\n",
      "Output:\n",
      "[[0.46386106 0.56386106 0.66386106 0.76386106]\n",
      " [0.4930926  0.5930926  0.6930926  0.7930926 ]]\n",
      "[[ 1.  5.]\n",
      " [ 4. 11.]]\n",
      "[[1.09932072]\n",
      " [1.01757194]]\n",
      "Output:\n",
      "[[0.46386106 0.56386106 0.66386106 0.76386106]\n",
      " [0.4930926  0.5930926  0.6930926  0.7930926 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "Q = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "K = np.array([[1, 0, 0], [0, 1, 1]])\n",
    "V = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "\n",
    "output = attention_1(Q, K, V)\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = attention_2(Q, K, V)\n",
    "print(\"Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Batch_size as an additional dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched scaled dot product attention\n",
    "# Shape of Q: [batch_size, length of sequence or num_patches, num_features]\n",
    "# Shape of K: [batch_size, length of sequence or num_patches, num_features]\n",
    "# Shape of V: [batch_size, length of sequence or num_patches, num_features_d]\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    batch_size, num_patches, num_features = Q.shape\n",
    "    \n",
    "    # 1. dot product Q and K^T to compute similarity scores\n",
    "    # Kt = K.transpose(1,2)     # PyTorch code, out shape: [batch_size, num_features, num_patches]\n",
    "    Kt = np.transpose(K, (0,2,1)) # numpy code, out shape: [batch_size, num_features, num_patches]\n",
    "    # print(Kt.shape)\n",
    "    # Computer attention score\n",
    "    attention_score = Q@Kt / np.sqrt(num_features) # [batch_size, num_patches, num_patches]\n",
    "    \n",
    "    # 2. Apply softmax to get the attention weights\n",
    "    attention_weights = softmax_np(attention_score) # [batch_size, num_patches, num_patches]\n",
    "    \n",
    "    # 3. Use attention weights to weigh the values V\n",
    "    output = attention_weights@V # (b,n,n) (b,n,d) = (b,n,d)\n",
    "    return output # [batch_size, num_patches, num_features_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3) (2, 2, 3) (2, 2, 4)\n",
      "(2, 3, 2)\n",
      "[[[1.09932072]\n",
      "  [1.01757194]]\n",
      "\n",
      " [[1.09932072]\n",
      "  [1.01757194]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.46386106, 0.56386106, 0.66386106, 0.76386106],\n",
       "        [0.4930926 , 0.5930926 , 0.6930926 , 0.7930926 ]],\n",
       "\n",
       "       [[0.46386106, 0.56386106, 0.66386106, 0.76386106],\n",
       "        [0.4930926 , 0.5930926 , 0.6930926 , 0.7930926 ]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "Q = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n",
    "K = np.array([[[1, 0, 0], [0, 1, 1]], [[1, 0, 0], [0, 1, 1]]])\n",
    "V = np.array([[[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]], [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]])\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)\n",
    "output = scaled_dot_product_attention(Q, K, V)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MultiHead attention\n",
    "\n",
    "<div>\n",
    "<img src=\"images/MultiHeadAttention_v2.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class ScaledDotProductAttention_torch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention_torch, self).__init__()\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        # input is 4 dimensional tensor\n",
    "        # [batch_size, n_heads, length, head_dim]\n",
    "        batch_size, n_heads, length, head_dim = Q.shape\n",
    "        \n",
    "        # 1. dot product Q and K^T to compute similarity scores\n",
    "        Kt = K.transpose(2,3) # PyTorch (batch_size, n_heads, head_dim, length)\n",
    "        # Computer attention score\n",
    "        attention_score = Q@Kt / np.sqrt(head_dim) # (batch_size, n_heads, length, length)\n",
    "        # 2. Apply softmax to get the attention weights\n",
    "        attention_weights = softmax_torch(attention_score) # (batch_size, n_heads, length, length)\n",
    "        # 3. Use attention weights to weigh the values V\n",
    "        output = attention_weights@V # (b,n,length,length) x (b,n,length,head_dim)\n",
    "        return output # (batch_size, n_heads, length, head_dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.module):\n",
    "    def __init__(self, n_heads, n_features):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_features = n_features # d_model\n",
    "        self.head_dim = n_features // n_heads\n",
    "        self.Wq = nn.Linear(n_features, n_features)\n",
    "        self.Wk = nn.Linear(n_features, n_features)\n",
    "        self.Wv = nn.Linear(n_features, n_features)\n",
    "        self.Wo = nn.Linear(n_features, n_features)\n",
    "    \n",
    "    def forward(self, Q, K, V): #todo: why not use the same input for Q, K, V?\n",
    "        # input is three dimensional tensor\n",
    "        # Q: [batch_size, length, n_features]\n",
    "        # K: [batch_size, length, n_features]\n",
    "        # V: [batch_size, length, n_features]\n",
    "        b, n, m = Q.shape\n",
    "        # 1. Linearly project Q, K, V\n",
    "        Q = self.Wq(Q)\n",
    "        K = self.Wk(K)\n",
    "        V = self.Wv(V)\n",
    "        # 2. Split into multiple heads\n",
    "        Q = Q.reshape(b, n, self.n_heads, self.head_dim)\n",
    "        K = K.reshape(b, n, self.n_heads, self.head_dim)\n",
    "        V = V.reshape(b, n, self.n_heads, self.head_dim)\n",
    "        # 3. Transpose to get dimensions (b, n_heads, n, head_dim)\n",
    "        Q = Q.transpose(1,2) # b,n_heads,n,head_dim\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "        # 4. Apply scaled dot product attention\n",
    "        # (batch_size, n_heads, length, head_dim)\n",
    "        output = ScaledDotProductAttention_torch(Q, K, V)\n",
    "        # 5. Concatenate the heads\n",
    "        output = output.transpose(1,2).reshape(b, n, self.n_features) #\n",
    "        # 6. Linearly project the concatenated heads\n",
    "        output = self.Wo(output) # b,n,n_features\n",
    "        return output #b,n,n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3248e+00,  4.1775e-01, -1.1600e+00],\n",
       "         [-9.8029e-01, -3.9965e-01, -8.1531e-01],\n",
       "         [ 9.1944e-01,  5.5010e-01, -6.3888e-01],\n",
       "         [ 1.6931e+00,  1.3687e+00,  1.5330e-01],\n",
       "         [-1.0919e+00, -2.9230e-01, -5.5353e-01]],\n",
       "\n",
       "        [[ 6.2891e-01,  1.3182e+00,  8.5776e-01],\n",
       "         [ 5.3094e-01,  3.6238e-01, -6.6410e-01],\n",
       "         [ 9.9441e-04,  2.4564e-02, -8.3489e-01],\n",
       "         [ 1.3581e+00,  4.3562e-01, -1.1550e+00],\n",
       "         [-1.3422e-01,  4.4799e-02, -7.0092e-01]],\n",
       "\n",
       "        [[ 6.8076e-01,  4.8757e-01, -5.6631e-01],\n",
       "         [ 1.2461e+00,  9.2266e-01, -2.5946e-01],\n",
       "         [ 1.6625e-01,  9.5595e-01,  5.9633e-01],\n",
       "         [ 2.6951e-01,  7.1117e-01,  1.1147e-01],\n",
       "         [ 1.7943e+00,  1.3612e+00,  6.5730e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(3, 5, 2)\n",
    "aa = nn.Linear(2, 3)\n",
    "aa(input) # 3,5,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3248e+00,  4.1775e-01, -1.1600e+00],\n",
       "         [-9.8029e-01, -3.9965e-01, -8.1531e-01],\n",
       "         [ 9.1944e-01,  5.5010e-01, -6.3888e-01],\n",
       "         [ 1.6931e+00,  1.3687e+00,  1.5330e-01],\n",
       "         [-1.0919e+00, -2.9230e-01, -5.5353e-01]],\n",
       "\n",
       "        [[ 6.2891e-01,  1.3182e+00,  8.5776e-01],\n",
       "         [ 5.3094e-01,  3.6238e-01, -6.6410e-01],\n",
       "         [ 9.9441e-04,  2.4564e-02, -8.3489e-01],\n",
       "         [ 1.3581e+00,  4.3562e-01, -1.1550e+00],\n",
       "         [-1.3422e-01,  4.4799e-02, -7.0092e-01]],\n",
       "\n",
       "        [[ 6.8076e-01,  4.8757e-01, -5.6631e-01],\n",
       "         [ 1.2461e+00,  9.2266e-01, -2.5946e-01],\n",
       "         [ 1.6625e-01,  9.5595e-01,  5.9633e-01],\n",
       "         [ 2.6951e-01,  7.1117e-01,  1.1147e-01],\n",
       "         [ 1.7943e+00,  1.3612e+00,  6.5730e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input@aa.weight.T + aa.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "K = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "print(Q)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0],\n",
       "       [3, 4, 0],\n",
       "       [5, 6, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Q, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0],\n",
       "       [3, 4, 0],\n",
       "       [5, 6, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0],\n",
       "        [3, 4, 0],\n",
       "        [5, 6, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qt = torch.tensor(Q)\n",
    "Kt = torch.tensor(K)\n",
    "\n",
    "torch.matmul(Qt, Kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0],\n",
       "        [3, 4, 0],\n",
       "        [5, 6, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(Qt, Kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0],\n",
       "        [3, 4, 0],\n",
       "        [5, 6, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qt @ Kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi30k, Multi30k\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
