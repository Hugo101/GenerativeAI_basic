{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Pytorch implementation of Transformer from scratch Demo**\n","\n","Note: Many codes and figures are borrowed from:\n","- https://github.com/BoXiaolei/MyTransformer_pytorch/blob/main/MyTransformer.ipynb\n","- https://nn.labml.ai/index.html \n","- https://github.com/hyunwoongko/transformer \n","- https://jalammar.github.io/illustrated-transformer/\n","- https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math \n","import torch.utils.data as data\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/transformer.png\" width=\"800\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["# Positional Encoding"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Mask"]},{"cell_type":"markdown","metadata":{},"source":["## Pading Mask"]},{"cell_type":"markdown","metadata":{},"source":["In the Transformer architecture, a **padding mask** is used to handle **sequences of different lengths**. Here's why it's important:\n","\n","- **Variable Sequence Lengths**: In many natural language processing tasks, input sequences (like sentences) vary in length. However, neural networks, including Transformers, typically require inputs of a fixed size. To manage this, shorter sequences are often padded with special tokens (like [PAD]) to match the length of the longest sequence in a batch.\n","\n","- Ignoring Padding Tokens during Training: The padding tokens are not actual data; they're just placeholders. It's crucial that the model doesn't treat these padding tokens as meaningful input. The padding mask is a mechanism to ensure that the model ignores these tokens during training and inference. It does this by zeroing out (masking) the padding tokens' impact on the model's output.\n","\n","- Attention Mechanism Efficiency: Transformers use an attention mechanism to weigh the importance of different parts of the input sequence. Without a padding mask, the attention mechanism might incorrectly assign significance to the padding tokens, leading to less accurate or meaningful outputs.\n","\n","- Preventing Data Leakage: In certain cases, especially in language modeling tasks, padding at the beginning or end of sequences might inadvertently reveal information about the sequence. A mask helps ensure that the model's predictions are based solely on actual data, not on these artificial padding tokens.\n","\n","#### In summary, the pad mask in Transformer architectures is a critical component for handling variable-length input sequences effectively and ensuring that the model's attention mechanism focuses on the meaningful parts of the input, thereby improving the quality and relevance of the model's outputs."]},{"cell_type":"markdown","metadata":{},"source":["- When is this calculated mask used?\n","\n","After the multiplication of query and key's transpose, resulting in the attention score matrix of size (len_q,len_k), the mask obtained from this function is used to cover the results of the matrix multiplication. In the original multiplication result matrix (len_q,len_k), the meaning of the element in the ith row and jth column is \"the attention score of the ith word in the q sequence to the jth word in the k sequence\". The entire ith row represents the attention of this word in q to all words in k, and the entire jth column represents the attention of all words in q to the jth word in k. As padding, none of the words in q should pay attention to it, hence the corresponding columns should be set to True.\n","\n","- Why is only the padding position of k masked, and not that of q? (i.e., why is it that only the last few columns of the return matrix of this function are True, and not the last few rows as well?)\n","\n","Logically, it should be like this: as padding, it should neither attract attention nor pay attention to others. The attention that the padding calculates towards other words is also meaningless. Here, we are actually cutting corners, but this is because: the attention of the padding in q to the words in k is not going to be used, as we won't use a padding character to predict the next word. Moreover, its vector representation, no matter how it's updated, will not affect the calculations of other words in q, so we let it be. However, the padding in k is different. If it's not managed, it will meaninglessly absorb a lot of attention from the words in q, leading to biases in the model's learning."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# The sentences we input into the model vary in length, and we use a placeholder 'P' to pad them to the length of the longest sentence. These placeholders are meaningless and we set these positions to True. The following function returns a Boolean tensor indicating whether the position is a placeholder.\n","# Return: tensor [batch_size, len_q, len_k]，True means the position is a placeholder\n","\n","def get_attn_pad_mask(seq_q, seq_k):\n","    batch_size, len_q = seq_q.size()\n","    _,          len_k = seq_k.size()\n","    # seq_k.data.eq(0):，element in seq_k will be True (if ele == 0), False otherwise.\n","    # broadcast\n","    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # pad_attn_mask: [batch_size,1,len_k]\n","\n","    # To provide a k for each q, so the second dimension is expanded q times.\n","    # Expand is not really doubling the memory, it just repeats the reference, and any modification to any reference will modify the original value.\n","    # Here we use it to save memory because we won't modify this mask.\n","    return pad_attn_mask.expand(batch_size, len_q, len_k) # return: [batch_size, len_q, len_k]\n","    # Return batch_size len_q * len_k matrix, content is True and False, True means the position is a placeholder.\n","    # The i-th row and the j-th column indicate whether the attention of the i-th word of the query to the j-th word of the key is meaningless. If it is meaningless, it is True. If it is meaningful, it is False (that is, the position of the padding is True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False]],\n","\n","        [[False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True]],\n","\n","        [[False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True]]])\n"]}],"source":["# test for get_attn_pad_mask\n","seq_q = torch.tensor([[1,2,3,0,0],[3,4,5,6,0],[2,3,0,0,0]])\n","seq_k = torch.tensor([[1,2,3,4,5],[1,2,0,0,0],[1,2,3,0,0]])\n","print(get_attn_pad_mask(seq_q, seq_k))"]},{"cell_type":"markdown","metadata":{},"source":["## Subsequence Mask"]},{"cell_type":"markdown","metadata":{},"source":["This mask is used in the first \"Masked Multi-Head self Attention\" module in Decoder of Transformer. The goal is to prevent the model seeing the future input.\n","\n","Take the example below:\n","\n","Assuming one sentence with 5 tokens as the decoder input. The i-th row and j-th column denotes the attention of i-th token to j-th token.\n","\n","For the i-th token (row), it can only see itself and tokens before it, and the tokens after it will be filtered. So 1 means filtering, and 0 means keeping."]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/subsequenceMask.png\" width=\"500\"/>\n","</div>"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# To prevent positions from attending to subsequent positions\n","def get_attn_subsequence_mask(seq):\n","    \"\"\"seq: [batch_size, tgt_len]\"\"\"\n","    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n","    # np.triu: Return a copy of a matrix with the elements below the k-th diagonal zeroed.\n","    # np.triu is to generate an upper triangular matrix, k is the offset relative to the main diagonal\n","    # k = 1 means not including the main diagonal (starting from the main diagonal offset 1\n","    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n","    subsequence_mask = torch.from_numpy(subsequence_mask).byte() \n","    # Because there are only 0 and 1, byte is used to save memory.\n","    return subsequence_mask  # return: [batch_size, tgt_len, tgt_len]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0, 1, 1, 1, 1],\n","         [0, 0, 1, 1, 1],\n","         [0, 0, 0, 1, 1],\n","         [0, 0, 0, 0, 1],\n","         [0, 0, 0, 0, 0]],\n","\n","        [[0, 1, 1, 1, 1],\n","         [0, 0, 1, 1, 1],\n","         [0, 0, 0, 1, 1],\n","         [0, 0, 0, 0, 1],\n","         [0, 0, 0, 0, 0]]], dtype=torch.uint8)\n"]}],"source":["# test for get_attn_subsequence_mask\n","seq = torch.tensor([[1,2,3,0,0],[1,2,3,4,0]])\n","print(get_attn_subsequence_mask(seq))"]},{"cell_type":"markdown","metadata":{},"source":["# Scaled Dot Product Attention\n","\n","<div>\n","<img src=\"images/ScaledDotProductAttention.png\" width=\"500\"/>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self):\n","        super(ScaledDotProductAttention, self).__init__()\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        '''\n","        Q: [batch_size, n_heads, len_q, d_k]\n","        K: [batch_size, n_heads, len_k, d_k]\n","        V: [batch_size, n_heads, len_v(=len_k), d_v] \n","        Two types of attention:\n","        1) self attention\n","        2) cross attention: K and V are encoder's output, so the shape of K and V are the same\n","        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n","        # todo: len_q and len_k may be different???\n","        '''\n","        batch_size, n_heads, len_q, d_k = Q.shape \n","        # 1) computer attention score QK^T/sqrt(d_k)\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores: [batch_size, n_heads, len_q, len_k]\n","        # 2) mask operation (option) and softmax\n","        # True positions in mask will be replaced with -1e9\n","        scores.masked_fill_(attn_mask, -1e9)\n","        attn = nn.Softmax(dim=-1)(scores)  # attn: [batch_size, n_heads, len_q, len_k]\n","        # 3) use attention weights to weigh the value V\n","        context = torch.matmul(attn, V)  # context: [batch_size, n_heads, len_q, d_v]\n","        '''\n","        得出的context是每个维度(d_1-d_v)都考虑了在当前维度(这一列)当前token对所有token的注意力后更新的新的值，\n","        换言之每个维度d是相互独立的，每个维度考虑自己的所有token的注意力，所以可以理解成1列扩展到多列\n","\n","        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n","        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n","        '''\n","        return context # context: [batch_size, n_heads, len_q, d_v]"]},{"cell_type":"markdown","metadata":{},"source":["# MultiHeadAttention\n","<div>\n","<img src=\"images/MultiHeadAttention.png\" width=\"300\"/>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads, dropout_rate):\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.dropout_rate = dropout_rate\n","        self.head_dim = d_model // n_heads\n","        self.W_Q = nn.Linear(d_model, d_model)\n","        self.W_K = nn.Linear(d_model, d_model)\n","        self.W_V = nn.Linear(d_model, d_model)\n","        self.scaled_dot_product_attention = ScaledDotProductAttention()\n","        self.W_O = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        '''\n","        Q: [batch_size, len_q, d_model]\n","        K: [batch_size, len_k, d_model]\n","        V: [batch_size, len_v, d_model] \n","        attn_mask: [batch_size, seq_len, seq_len]\n","        '''\n","        batch_size, len_q, d_model = Q.shape\n","        batch_size, len_k, d_model = K.shape\n","        batch_size, len_v, d_model = V.shape\n","        # 1) linear projection\n","        Q = self.W_Q(Q)\n","        K = self.W_K(K)\n","        V = self.W_V(V)\n","        # 2) split by heads\n","        # [batch_size, len_q, d_model] -> [batch_size, len_q, n_heads, head_dim]\n","        Q = Q.reshape(batch_size, len_q, self.n_heads, self.head_dim)\n","        K = K.reshape(batch_size, len_k, self.n_heads, self.head_dim)\n","        V = V.reshape(batch_size, len_v, self.n_heads, self.head_dim)\n","        # 3) transpose for attention dot product\n","        # [batch_size, len_q, n_heads, head_dim] -> [batch_size, n_heads, len_q, head_dim]\n","        Q = Q.transpose(1, 2)\n","        K = K.transpose(1, 2)\n","        V = V.transpose(1, 2)\n","        # 4) attention\n","        context = self.scaled_dot_product_attention(Q, K, V, attn_mask)\n","        # context: [batch_size, n_heads, len_q, head_dim]\n","        # 5) concat heads\n","        # method 1:\n","        output = context.transpose(1, 2).reshape(batch_size, len_q, self.d_model)\n","        # output: [batch_size, len_q, d_model]\n","        # method 2:\n","        # output = torch.cat([context[:,i,:,:] for i in range(self.n_heads)], dim=-1)\n","        # output: [batch_size, len_q, d_model]\n","        # 6) linear projection (concat heads)\n","        output = self.W_O(output)\n","        return output # output: [batch_size, len_q, d_model]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Feed-Forward Networks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout_rate):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.d_model = d_model\n","        self.d_ff = d_ff\n","        self.W_1 = nn.Linear(d_model, d_ff)\n","        self.W_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        '''\n","        x: [batch_size, seq_len, d_model]\n","        '''\n","        output = self.relu(self.W_1(x))\n","        output = self.W_2(output)\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder Layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Decoder"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Transformer"]},{"cell_type":"markdown","metadata":{},"source":["- Encoder\n","- Decoder\n","- Dense"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Demo of Training"]},{"cell_type":"markdown","metadata":{},"source":["- ## Dataset Preparation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# S: 起始标记\n","# E: 结束标记\n","# P：意为padding，将当前序列补齐至最长序列长度的占位符\n","sentence = [\n","    # enc_input   dec_input    dec_output\n","    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n","    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n","]\n","\n","# 词典，padding用0来表示\n","# 源词典\n","src_vocab = {'P':0, 'ich':1,'mochte':2,'ein':3,'bier':4,'cola':5}\n","src_vocab_size = len(src_vocab) # 6\n","# 目标词典（包含特殊符）\n","tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n","# 反向映射词典，idx ——> word\n","idx2word = {v:k for k,v in tgt_vocab.items()}\n","tgt_vocab_size = len(tgt_vocab) # 9\n","\n","src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n","tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 这个函数把原始输入序列转换成token表示\n","def make_data(sentence):\n","    enc_inputs, dec_inputs, dec_outputs = [],[],[]\n","    for i in range(len(sentence)):\n","        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n","        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n","        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n","        \n","        enc_inputs.append(enc_input)\n","        dec_inputs.append(dec_input)\n","        dec_outputs.append(dec_output)\n","        \n","    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n","    return torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)\n","\n","enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n","\n","print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n","print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n","print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 使用Dataset加载数据\n","class MyDataSet(data.Dataset):\n","    def __init__(self,enc_inputs, dec_inputs, dec_outputs):\n","        super(MyDataSet,self).__init__()\n","        self.enc_inputs = enc_inputs\n","        self.dec_inputs = dec_inputs\n","        self.dec_outputs = dec_outputs\n","        \n","    def __len__(self):\n","        # 我们前面的enc_inputs.shape = [2,5],所以这个返回的是2\n","        return self.enc_inputs.shape[0] \n","    \n","    # 根据idx返回的是一组 enc_input, dec_input, dec_output\n","    def __getitem__(self, idx):\n","        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n","\n","# 构建DataLoader\n","loader = data.DataLoader(dataset=MyDataSet(enc_inputs,dec_inputs, dec_outputs),batch_size=2,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["- ## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"HENN","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":2}
