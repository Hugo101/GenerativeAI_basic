{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Pytorch implementation of Transformer from scratch Demo**\n","\n","Note: Many codes and figures are borrowed from:\n","- https://github.com/BoXiaolei/MyTransformer_pytorch/blob/main/MyTransformer.ipynb\n","- https://nn.labml.ai/index.html \n","- https://github.com/hyunwoongko/transformer \n","- https://jalammar.github.io/illustrated-transformer/\n","- https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import math \n","import torch.utils.data as data\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/transformer.png\" width=\"800\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Positional Encoding"]},{"cell_type":"markdown","metadata":{},"source":["The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed"]},{"cell_type":"markdown","metadata":{},"source":["## Sinusoidal Positional Encoding"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model = 512, dropout = 0.1, max_len = 5000):\n","        '''\n","        d_model: dimension of the word embedding, 512\n","        max_len: the maximum number of tokens in a sentence, 5000\n","        '''\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        # Generate a max_len * d_model matrix, that is, 5000 * 512\n","        # 5000 is the maximum number of tokens in a sentence, \n","        # and 512 is the length of a token represented by a vector.\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # pos：[max_len,1]\n","        # First calculate the fraction in the brackets, pos is [5000,1], the denominator is [256],\n","        # and the result of multiplication by broadcasting is [5000,256]\n","        div_term = pos / pow(10000.0,torch.arange(0, d_model, 2).float() / d_model)\n","        pe[:, 0::2] = torch.sin(div_term)\n","        pe[:, 1::2] = torch.cos(div_term)\n","        # A sentence needs to do pe once, and there will be multiple sentences in a batch,\n","        # so add one dimension to do broadcast when adding to a batch of input data\n","        pe = pe.unsqueeze(0) # [5000,512] -> [1,5000,512] \n","        # English: register_buffer is used to save the parameters that will not be updated, and the parameters are saved in the buffer\n","        self.register_buffer('pe', pe)\n","        \n","        \n","    def forward(self, x):\n","        '''x: [batch_size, seq_len, d_model]'''\n","        # 5000 is the maximum seq_len we have defined, that is to say, we have calculated the pe for the most situation\n","        x = x + self.pe[:, :x.size(1), :]  # Note: residual connection\n","        return self.dropout(x) \n","        # return: [batch_size, seq_len, d_model]\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Learned Positional Encoding"]},{"cell_type":"markdown","metadata":{},"source":["Learned positional encoding assigns each element with a learned column vector which encodes its absolute position (Gehring, et al. 2017) and furthermroe this encoding can be learned differently per layer (Al-Rfou et al. 2018)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 2. Mask"]},{"cell_type":"markdown","metadata":{},"source":["## Pading Mask"]},{"cell_type":"markdown","metadata":{},"source":["In the Transformer architecture, a **padding mask** is used to handle **sequences of different lengths**. Here's why it's important:\n","\n","- **Variable Sequence Lengths**: In many natural language processing tasks, input sequences (like sentences) vary in length. However, neural networks, including Transformers, typically require inputs of a fixed size. To manage this, shorter sequences are often padded with special tokens (like [PAD]) to match the length of the longest sequence in a batch.\n","\n","- Ignoring Padding Tokens during Training: The padding tokens are not actual data; they're just placeholders. It's crucial that the model doesn't treat these padding tokens as meaningful input. The padding mask is a mechanism to ensure that the model ignores these tokens during training and inference. It does this by zeroing out (masking) the padding tokens' impact on the model's output.\n","\n","- Attention Mechanism Efficiency: Transformers use an attention mechanism to weigh the importance of different parts of the input sequence. Without a padding mask, the attention mechanism might incorrectly assign significance to the padding tokens, leading to less accurate or meaningful outputs.\n","\n","- Preventing Data Leakage: In certain cases, especially in language modeling tasks, padding at the beginning or end of sequences might inadvertently reveal information about the sequence. A mask helps ensure that the model's predictions are based solely on actual data, not on these artificial padding tokens.\n","\n","#### In summary, the pad mask in Transformer architectures is a critical component for handling variable-length input sequences effectively and ensuring that the model's attention mechanism focuses on the meaningful parts of the input, thereby improving the quality and relevance of the model's outputs."]},{"cell_type":"markdown","metadata":{},"source":["- When is this calculated mask used?\n","\n","After the multiplication of query and key's transpose, resulting in the attention score matrix of size (len_q,len_k), the mask obtained from this function is used to cover the results of the matrix multiplication. In the original multiplication result matrix (len_q,len_k), the meaning of the element in the ith row and jth column is \"the attention score of the ith word in the q sequence to the jth word in the k sequence\". The entire ith row represents the attention of this word in q to all words in k, and the entire jth column represents the attention of all words in q to the jth word in k. As padding, none of the words in q should pay attention to it, hence the corresponding columns should be set to True.\n","\n","- Why is only the padding position of k masked, and not that of q? (i.e., why is it that only the last few columns of the return matrix of this function are True, and not the last few rows as well?)\n","\n","Logically, it should be like this: as padding, it should neither attract attention nor pay attention to others. The attention that the padding calculates towards other words is also meaningless. Here, we are actually cutting corners, but this is because: the attention of the padding in q to the words in k is not going to be used, as we won't use a padding character to predict the next word. Moreover, its vector representation, no matter how it's updated, will not affect the calculations of other words in q, so we let it be. However, the padding in k is different. If it's not managed, it will meaninglessly absorb a lot of attention from the words in q, leading to biases in the model's learning."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# The sentences we input into the model vary in length, and we use a placeholder 'P' to pad them to the length of the longest sentence. These placeholders are meaningless and we set these positions to True. The following function returns a Boolean tensor indicating whether the position is a placeholder.\n","# Return: tensor [batch_size, len_q, len_k]，True means the position is a placeholder\n","\n","def get_attn_pad_mask(seq_q, seq_k):\n","    '''\n","    seq_q: [batch_size, len_q]\n","    seq_k: [batch_size, len_k]\n","    '''\n","    batch_size, len_q = seq_q.size()\n","    _,          len_k = seq_k.size()\n","    # seq_k.data.eq(0):，element in seq_k will be True (if ele == 0), False otherwise.\n","    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # pad_attn_mask: [batch_size,1,len_k]\n","\n","    # To provide a k for each q, so the second dimension is expanded q times.\n","    # Expand is not really doubling the memory, it just repeats the reference, and any modification to any reference will modify the original value.\n","    # Here we use it to save memory because we won't modify this mask.\n","    return pad_attn_mask.expand(batch_size, len_q, len_k) # return: [batch_size, len_q, len_k]\n","    # Return batch_size len_q * len_k matrix, content is True and False, True means the position is a placeholder.\n","    # The i-th row and the j-th column indicate whether the attention of the i-th word of the query to the j-th word of the key is meaningless. If it is meaningless, it is True. If it is meaningful, it is False (that is, the position of the padding is True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False],\n","         [False, False, False, False, False]],\n","\n","        [[False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True],\n","         [False, False,  True,  True,  True]],\n","\n","        [[False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True],\n","         [False, False, False,  True,  True]]])\n"]}],"source":["# test for get_attn_pad_mask\n","seq_q = torch.tensor([[1,2,3,4,0,0],[3,4,5,6,7,0],[2,3,4,0,0,0]])\n","seq_k = torch.tensor([[1,2,3,4,5],[1,2,0,0,0],[1,2,3,0,0]])\n","print(get_attn_pad_mask(seq_q, seq_k))"]},{"cell_type":"markdown","metadata":{},"source":["## Subsequence Mask"]},{"cell_type":"markdown","metadata":{},"source":["This mask is used in the first \"Masked Multi-Head self Attention\" module in Decoder of Transformer. The goal is to prevent the model seeing the future input.\n","\n","Take the example below:\n","\n","Assuming one sentence with 5 tokens as the decoder input. The i-th row and j-th column denotes the attention of i-th token to j-th token.\n","\n","For the i-th token (row), it can only see itself and tokens before it, and the tokens after it will be filtered. So 1 means filtering, and 0 means keeping."]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/subsequenceMask.png\" width=\"500\"/>\n","</div>"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0, 1, 1, 1, 1],\n","         [0, 0, 1, 1, 1],\n","         [0, 0, 0, 1, 1],\n","         [0, 0, 0, 0, 1],\n","         [0, 0, 0, 0, 0]],\n","\n","        [[0, 1, 1, 1, 1],\n","         [0, 0, 1, 1, 1],\n","         [0, 0, 0, 1, 1],\n","         [0, 0, 0, 0, 1],\n","         [0, 0, 0, 0, 0]]], dtype=torch.uint8)\n"]}],"source":["# To prevent positions from attending to subsequent positions\n","def get_attn_subsequence_mask(seq):\n","    \"\"\"\n","    seq: [batch_size, tgt_len]\n","    This is only used in decoder, so the length of seq is the length of target sentence.\n","    \"\"\"\n","    batch_size, tgt_len = seq.shape\n","    attn_shape = [batch_size, tgt_len, tgt_len]\n","    # np.triu: Return a copy of a matrix with the elements below the k-th diagonal zeroed.\n","    # np.triu is to generate an upper triangular matrix, k is the offset relative to the main diagonal\n","    # k = 1 means not including the main diagonal (starting from the main diagonal offset 1\n","    # subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n","    # subsequence_mask = torch.from_numpy(subsequence_mask).byte() \n","    subsequence_mask = torch.triu(torch.ones(attn_shape), diagonal=1).byte() #.byte() is equivalent to .to(torch.uint8)\n","    # Because there are only 0 and 1, byte is used to save memory.\n","    return subsequence_mask  # return: [batch_size, tgt_len, tgt_len]\n","\n","### test for get_attn_subsequence_mask\n","seq = torch.tensor([[1,2,3,0,0],[1,2,3,4,0]])\n","print(get_attn_subsequence_mask(seq))"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Scaled Dot Product Attention"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/ScaledDotProductAttention.png\" width=\"500\"/>\n","</div>"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self):\n","        super(ScaledDotProductAttention, self).__init__()\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        '''\n","        Q: [batch_size, n_heads, len_q, d_k]  \n","        K: [batch_size, n_heads, len_k, d_k]\n","        V: [batch_size, n_heads, len_v(=len_k), d_v] \n","        Two types of attention:\n","        1) self attention\n","        2) cross attention: K and V are encoder's output, so the shape of K and V are the same\n","        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n","        \n","        # len_q is not necessary to be euqal to len_k\n","        because len_q is the length of the query sentence (decoder input, or predicted sentence in the 2) attention operation),\n","        and len_k is the length of the key sentence (encoder input, or source sentence).\n","        '''\n","        batch_size, n_heads, len_q, d_k = Q.shape \n","        # 1) computer attention score QK^T/sqrt(d_k)\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores: [batch_size, n_heads, len_q, len_k]\n","        # 2) mask operation (option), only used in the decoder\n","        if attn_mask is not None:\n","            scores.masked_fill_(attn_mask, float('-inf')) # or float('-inf'), -1e9: negative infinity\n","            # Fills elements of self tensor with value where mask is True.\n","            # The masked elements in the scores are replaced by -1e9, \n","            # so that the softmax operation will make the value of the masked position close to 0.\n","\n","        # 3) softmax to get attention weights\n","        attn = nn.Softmax(dim=-1)(scores)  # attn: [batch_size, n_heads, len_q, len_k]\n","        # 4) use attention weights to weigh the value V\n","        context = torch.matmul(attn, V)  # context: [batch_size, n_heads, len_q, d_v]\n","        '''\n","        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n","        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n","        '''\n","        return context"]},{"cell_type":"markdown","metadata":{},"source":["# 4. MultiHeadAttention"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/MultiHeadAttention_v2.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model = 512, n_heads = 8, dropout_rate = 0.0):\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.dropout_rate = dropout_rate\n","        self.head_dim = d_model // n_heads\n","        self.W_Q = nn.Linear(d_model, d_model)\n","        self.W_K = nn.Linear(d_model, d_model)\n","        self.W_V = nn.Linear(d_model, d_model)\n","        self.scaled_dot_product_attention = ScaledDotProductAttention()\n","        self.W_O = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        '''\n","        Q: [batch_size, len_q, d_model]\n","        K: [batch_size, len_k, d_model]\n","        V: [batch_size, len_v, d_model] \n","        attn_mask: [batch_size, seq_len, seq_len]\n","        '''\n","        batch_size, len_q, d_model = Q.shape\n","        batch_size, len_k, d_model = K.shape\n","        batch_size, len_v, d_model = V.shape\n","        \n","        # 1) linear projection\n","        Q = self.W_Q(Q)\n","        K = self.W_K(K)\n","        V = self.W_V(V)\n","        \n","        # 2) split by heads\n","        # [batch_size, len_q, d_model] -> [batch_size, len_q, n_heads, head_dim]\n","        Q = Q.reshape(batch_size, len_q, self.n_heads, self.head_dim)\n","        K = K.reshape(batch_size, len_k, self.n_heads, self.head_dim)\n","        V = V.reshape(batch_size, len_v, self.n_heads, self.head_dim)\n","        \n","        # 3) transpose for attention dot product\n","        # [batch_size, len_q, n_heads, head_dim] -> [batch_size, n_heads, len_q, head_dim]\n","        Q = Q.transpose(1, 2)\n","        K = K.transpose(1, 2)\n","        V = V.transpose(1, 2)\n","        \n","        # 4) attention\n","        # attn_mask: [batch_size, seq_len, seq_len] -> [batch_size, n_heads, len_q, len_k]\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n","        # attn_mask = attn_mask.unsqueeze(1).expand(-1, self.n_heads, -1, -1)\n","        context = self.scaled_dot_product_attention(Q, K, V, attn_mask)\n","        # context: [batch_size, n_heads, len_q, head_dim]\n","        \n","        # 5) concat heads\n","        # method 1:\n","        output = context.transpose(1, 2).reshape(batch_size, len_q, self.d_model)\n","        # output: [batch_size, len_q, d_model]\n","        \n","        # method 2:\n","        # output = torch.cat([context[:,i,:,:] for i in range(self.n_heads)], dim=-1)\n","        # output: [batch_size, len_q, d_model]\n","        \n","        # 6) linear projection (concat heads)\n","        output = self.W_O(output)\n","        return output # output: [batch_size, len_q, d_model]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 5. Feed-Forward Networks"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, \n","                 d_model = 512, \n","                 d_ff = 2048, \n","                 dropout_rate = 0.0):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.d_model = d_model\n","        self.d_ff = d_ff # dimension of latent layer for feed forward neural network\n","        self.W_1 = nn.Linear(d_model, d_ff)\n","        self.W_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        '''\n","        x: [batch_size, seq_len, d_model]\n","        '''\n","        output = self.relu(self.W_1(x))\n","        output = self.W_2(output)\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# 5. LayerNorm"]},{"cell_type":"markdown","metadata":{},"source":["For each sample, normalize across features."]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/LayerNorm.png\" width=\"700\"/>\n","</div>"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, d_model, eps=1e-12):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(d_model))\n","        self.beta = nn.Parameter(torch.zeros(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        \"\"\"_summary_\n","\n","        Args:\n","            x (_type_): _description_\n","\n","        Returns:\n","            _type_: _description_\n","        \"\"\"\n","        mean = x.mean(-1, keepdim=True)\n","        var = x.var(-1, unbiased=False, keepdim=True)\n","        # '-1' means last dimension. \n","\n","        out = (x - mean) / torch.sqrt(var + self.eps)\n","        out = self.gamma * out + self.beta\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Encoder"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder Layer"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/encoder.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 d_model = 512, \n","                 d_ff = 2048, \n","                 n_heads = 8, \n","                 dropout_rate = 0.0):\n","        super(EncoderLayer, self).__init__()\n","        self.enc_self_attn = MultiHeadAttention(d_model, n_heads, dropout_rate)\n","        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n","        self.layer_norm1 = LayerNorm(d_model)\n","        self.layer_norm2 = LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        \n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        '''\n","        enc_inputs:         [batch_size, src_len, d_model]\n","        enc_self_attn_mask: [batch_size, src_len, src_len]\n","        '''\n","        # Sublayer 1: self attention\n","        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n","        # enc_outputs: [batch_size, src_len, d_model]\n","        \n","        # add & norm\n","        enc_outputs = self.layer_norm1(enc_inputs + enc_outputs)\n","        # enc_outputs: [batch_size, src_len, d_model]\n","        \n","        # Sublayer 2: position-wise feed forward network\n","        enc_ff_outputs = self.pos_ffn(enc_outputs)\n","        # enc_ff_outputs: [batch_size, src_len, d_model]\n","        \n","        # add & norm\n","        enc_outputs = self.layer_norm2(enc_outputs + enc_ff_outputs)\n","        # enc_outputs: [batch_size, src_len, d_model]\n","        \n","        return enc_outputs"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/EncoderDecoder_2.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        d_model = 512, \n","        d_ff = 2048, \n","        n_heads = 8, \n","        n_layers = 6,\n","        dropout_rate = 0.0, \n","        ):\n","        super(Encoder, self).__init__()\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, n_heads, dropout_rate) for _ in range(n_layers)])\n","    \n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        '''\n","        enc_inputs: [batch_size, src_len, d_model]\n","        '''\n","        enc_outputs = enc_inputs\n","        # encoding\n","        for layer in self.layers:\n","            enc_outputs = layer(enc_outputs, enc_self_attn_mask)\n","        # enc_outputs: [batch_size, src_len, d_model]\n","        \n","        return enc_outputs"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Decoder"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Layer"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/decoder.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","class DecoderLayer(nn.Module):\n","    def __init__(self, \n","                 d_model = 512, \n","                 d_ff = 2048, \n","                 n_heads = 8, \n","                 dropout_rate = 0.1):\n","        super(DecoderLayer, self).__init__()\n","        self.dec_self_attn = MultiHeadAttention(d_model, n_heads, dropout_rate)\n","        self.dec_cros_attn = MultiHeadAttention(d_model, n_heads, dropout_rate)\n","        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n","        self.layer_norm_1 = LayerNorm(d_model)\n","        self.layer_norm_2 = LayerNorm(d_model)\n","        self.layer_norm_3 = LayerNorm(d_model)\n","        \n","    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n","        '''\n","        dec_inputs:         [batch_size, tgt_len, d_model]\n","        enc_outputs:        [batch_size, src_len, d_model]\n","        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n","        dec_enc_attn_mask:  [batch_size, tgt_len, src_len]\n","        '''\n","        # Sublayer 1: self attention\n","        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n","        # dec_outputs: [batch_size, tgt_len, d_model]\n","        \n","        # add & norm\n","        dec_outputs = self.layer_norm_1(dec_inputs + dec_outputs)\n","        # dec_outputs: [batch_size, tgt_len, d_model]\n","        \n","        # Sublayer 2: encoder-decoder cross attention, \n","        # Q (decoder), K (encoder output), V (encoder output)\n","        dec_outputs_2 = self.dec_cros_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n","        # dec_outputs: [batch_size, tgt_len, d_model]\n","        \n","        # add & norm\n","        dec_outputs_2 = self.layer_norm_2(dec_outputs + dec_outputs_2)\n","        # dec_outputs_2: [batch_size, tgt_len, d_model]\n","        \n","        # Sublayer 3: position-wise feed forward network\n","        dec_outputs_3 = self.pos_ffn(dec_outputs_2)\n","        # dec_ff_outputs: [batch_size, tgt_len, d_model]\n","        \n","        # add & norm\n","        dec_outputs_3 = self.layer_norm_3(dec_outputs_2 + dec_outputs_3)\n","        # dec_outputs_3: [batch_size, tgt_len, d_model]\n","        \n","        return dec_outputs_3 "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"markdown","metadata":{},"source":["- nn.Embedding\n","- PositionalEncoding\n","- 6 DecoderLayer"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/EncoderDecoder_2.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","class Decoder(nn.Module):\n","    def __init__(self, \n","                 d_model = 512, \n","                 d_ff = 2048, \n","                 n_heads = 8, \n","                 n_layers = 6,\n","                 dropout_rate = 0.1):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, d_ff, n_heads, dropout_rate) for _ in range(n_layers)])\n","    \n","    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_self_attn_subsequence_mask, memory_self_attn_mask):\n","        '''\n","        dec_inputs:         [batch_size, tgt_len]\n","        enc_outputs:        [batch_size, src_len, d_model]\n","        sometimes, enc_outputs is also called memory in the paper\n","        '''\n","        # combine two masks in decoder self attention\n","        dec_self_attn_mask = torch.gt((dec_self_attn_mask + dec_self_attn_subsequence_mask), 0)\n","        # dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n","        \n","        output = dec_inputs\n","        # decoding\n","        for layer in self.layers:\n","            output = layer(output, enc_outputs, dec_self_attn_mask, memory_self_attn_mask)\n","        # dec_outputs: [batch_size, tgt_len, d_model]\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# 9. Transformer"]},{"cell_type":"markdown","metadata":{},"source":["- Encoder\n","- Decoder\n","- Dense"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","<img src=\"images/EncoderDecoder_1.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens): # tokens: [batch_size, seq_len]\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","\n","# The Transformer model\n","class Transformer(nn.Module):\n","    def __init__(self, \n","                 src_vocab_size, \n","                 tgt_vocab_size, \n","                 d_model = 512, \n","                 d_ff = 2048, \n","                 n_heads = 8, \n","                 n_layers = 6,\n","                 dropout_rate = 0.1):\n","        super(Transformer, self).__init__()\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, d_model) #[batch_size, src_len, d_model]\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, d_model) #[batch_size, tgt_len, d_model]\n","        self.pos_embedding = PositionalEncoding(d_model, dropout_rate)\n","\n","        self.Encoder = Encoder(d_model, d_ff, n_heads, n_layers, dropout_rate)\n","        self.Decoder = Decoder(d_model, d_ff, n_heads, n_layers, dropout_rate)\n","        self.generator = nn.Linear(d_model, tgt_vocab_size)\n","        \n","    def create_masks(self, src, tgt):\n","        '''\n","        src: [batch_size, src_len]\n","        tgt: [batch_size, tgt_len]\n","        \n","        ''' \n","        # padding mask for encoder self attention: enc_self_attn_pad_mask\n","        src_key_padding_mask = get_attn_pad_mask(src, src) #[batch_size, src_len, src_len]\n","        \n","        # padding mask for decoder self attention: dec_self_attn_pad_mask\n","        tgt_key_padding_mask = get_attn_pad_mask(tgt, tgt) # [batch_size, tgt_len, tgt_len]\n","        \n","        # encoder-decoder cross attention mask: cross_attn_mask\n","        memory_key_padding_mask = get_attn_pad_mask(tgt, src) #[batch_size, tgt_len, src_len]\n","        \n","        # sequence mask (only exists in decoder)\n","        tgt_mask = get_attn_subsequence_mask(tgt) # [batch_size, tgt_len, tgt_len]\n","        \n","        if tgt_key_padding_mask.is_cuda:\n","            tgt_mask = tgt_mask.cuda()\n","        \n","        return  tgt_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask\n","\n","\n","    def forward(self, enc_inputs, dec_inputs):\n","        '''\n","        enc_inputs: [batch_size, src_len]\n","        dec_inputs: [batch_size, tgt_len]\n","        '''\n","        # embedding + position encoding\n","        src_emb = self.pos_embedding(self.src_tok_emb(enc_inputs))\n","        tgt_emb = self.pos_embedding(self.tgt_tok_emb(dec_inputs))\n","        \n","        # prepare masks\n","        tgt_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask = self.create_masks(enc_inputs, dec_inputs)\n","        \n","        # encoding\n","        enc_outputs = self.Encoder(src_emb, src_key_padding_mask)\n","        # enc_outputs: [batch_size, src_len, d_model]\n","        \n","        # decoding\n","        dec_outputs = self.Decoder(tgt_emb, enc_outputs, tgt_key_padding_mask, tgt_mask, memory_key_padding_mask)\n","        # dec_outputs: [batch_size, tgt_len, d_model]\n","        \n","        # projection\n","        dec_logits = self.generator(dec_outputs)\n","        # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n","        \n","        return dec_logits.view(-1, dec_logits.size(-1))  #  [batch_size * tgt_len, tgt_vocab_size]\n","        # there are batch_size sentences in one batch\n","        # first tgt_len words are the prediction probability of the first sentence,\n","        # then the next tgt_len words are the prediction probability of the second sentence, and so on."]},{"cell_type":"markdown","metadata":{},"source":["# 10. Demo of Training"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Preparation "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# S: Start of sentence\n","# E: End of sentence\n","# P: padding，make sure the length of the sentence is the same\n","\n","sentence = [\n","    # enc_input   dec_input    dec_output\n","    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n","    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n","]\n","# source vocab\n","src_vocab = {'P':0, 'ich':1,'mochte':2,'ein':3,'bier':4,'cola':5}\n","src_vocab_size = len(src_vocab) # 6\n","\n","# target vocab (including special symbols)\n","tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n","\n","# reverse mapping dictionary, idx ——> word\n","idx2word = {v:k for k,v in tgt_vocab.items()}\n","tgt_vocab_size = len(tgt_vocab) # 9\n","\n","src_len = 5\n","# the length of the longest sentence in the input sequence, \n","# which is actually the number of tokens in the longest sentence\n","tgt_len = 6\n","# the length of the longest sentence in the dec_input/dec_output sequence"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" enc_inputs: \n"," tensor([[1, 2, 3, 4, 0],\n","        [1, 2, 3, 5, 0]])\n"," dec_inputs: \n"," tensor([[6, 1, 2, 3, 4, 8],\n","        [6, 1, 2, 3, 5, 8]])\n"," dec_outputs: \n"," tensor([[1, 2, 3, 4, 8, 7],\n","        [1, 2, 3, 5, 8, 7]])\n"]}],"source":["# 这个函数把原始输入序列转换成token表示\n","def make_data(sentence):\n","    enc_inputs, dec_inputs, dec_outputs = [],[],[]\n","    for i in range(len(sentence)):\n","        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n","        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n","        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n","        \n","        enc_inputs.append(enc_input)\n","        dec_inputs.append(dec_input)\n","        dec_outputs.append(dec_output)\n","        \n","    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n","    return torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)\n","\n","enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n","\n","print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n","print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n","print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["class MyDataSet(data.Dataset):\n","    def __init__(self,enc_inputs, dec_inputs, dec_outputs):\n","        super(MyDataSet,self).__init__()\n","        self.enc_inputs = enc_inputs\n","        self.dec_inputs = dec_inputs\n","        self.dec_outputs = dec_outputs\n","        \n","    def __len__(self):\n","        # in the example above, enc_inputs.shape = [2,5], so return 2\n","        return self.enc_inputs.shape[0] \n","    \n","    # return a set of enc_input, dec_input, dec_output, according to the index\n","    def __getitem__(self, idx):\n","        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n","\n","# DataLoader\n","loader = data.DataLoader(dataset=MyDataSet(enc_inputs,dec_inputs, dec_outputs),batch_size=2,shuffle=True)\n","len(loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 2.3747642040252686\n","Epoch [2/10], Loss: 2.3079771995544434\n","Epoch [3/10], Loss: 1.9671508073806763\n","Epoch [4/10], Loss: 1.7235007286071777\n","Epoch [5/10], Loss: 1.407813549041748\n","Epoch [6/10], Loss: 1.1282033920288086\n","Epoch [7/10], Loss: 0.8996966481208801\n","Epoch [8/10], Loss: 0.6844269633293152\n","Epoch [9/10], Loss: 0.5365076661109924\n","Epoch [10/10], Loss: 0.369119793176651\n"]}],"source":["\n","model = Transformer(src_vocab_size, \n","                    tgt_vocab_size, \n","                    d_model = 512, \n","                    d_ff = 2048, \n","                    n_heads = 8, \n","                    n_layers = 6,\n","                    dropout_rate = 0.1).cuda()\n","model.train()\n","# 损失函数,忽略为0的类别不对其计算loss（因为是padding无意义）\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n","\n","# training\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for enc_inputs, dec_inputs, dec_outputs_true in loader:\n","        '''\n","        enc_inputs: [batch_size, src_len] [2,5]\n","        dec_inputs: [batch_size, tgt_len] [2,6]\n","        dec_outputs_true: [batch_size, tgt_len] [2,6]\n","        '''\n","        enc_inputs, dec_inputs, dec_outputs_true = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs_true.cuda()\n","        outputs = model(enc_inputs, dec_inputs) # outputs: [batch_size * tgt_len, tgt_vocab_size]\n","\n","        # print(\"pred: \", outputs.shape, outputs)\n","        # print(\"true: \", dec_outputs_true.view(-1).shape, dec_outputs_true.view(-1))\n","        loss = criterion(outputs, dec_outputs_true.view(-1))  # 将flatten dec_outputs_true作为target\n","\n","        # weight updates\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n","        # break\n","\n","torch.save(model, 'MyTransformer.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Model Inference"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"forward() missing 1 required positional argument: 'enc_self_attn_mask'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m enc_inputs \u001b[38;5;241m=\u001b[39m enc_inputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(enc_inputs)):\n\u001b[0;32m---> 38\u001b[0m     greedy_dec_input \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_symbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_vocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     predict  \u001b[38;5;241m=\u001b[39m model(enc_inputs[i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), greedy_dec_input) \u001b[38;5;66;03m# predict: [batch_size * tgt_len, tgt_vocab_size]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     predict \u001b[38;5;241m=\u001b[39m predict\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n","Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mgreedy_decoder\u001b[0;34m(model, enc_input, start_symbol)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreedy_decoder\u001b[39m(model, enc_input, start_symbol):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124;03m\"\"\"enc_input: [1, seq_len] 对应一句话\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     enc_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# enc_outputs: [1, seq_len, 512]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 生成一个1行0列的，和enc_inputs.data类型相同的空张量，待后续填充\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     dec_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(enc_input\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;66;03m# .data避免影响梯度信息\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/HENN/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'enc_self_attn_mask'"]}],"source":["# 原文使用的是大小为4的beam search，这里为简单起见使用更简单的greedy贪心策略生成预测，不考虑候选，每一步选择概率最大的作为输出\n","# 如果不使用greedy_decoder，那么我们之前实现的model只会进行一次预测得到['i']，并不会自回归，所以我们利用编写好的Encoder-Decoder来手动实现自回归（把上一次Decoder的输出作为下一次的输入，直到预测出终止符）\n","def greedy_decoder(model, enc_input, start_symbol):\n","    \"\"\"enc_input: [1, seq_len] 对应一句话\"\"\"\n","    enc_outputs = model.Encoder(enc_input) # enc_outputs: [1, seq_len, 512]\n","    # 生成一个1行0列的，和enc_inputs.data类型相同的空张量，待后续填充\n","    dec_input = torch.zeros(1, 0).type_as(enc_input.data) # .data避免影响梯度信息\n","    next_symbol = start_symbol\n","    flag = True\n","    while flag:\n","        # dec_input.detach() 创建 dec_input 的一个分离副本\n","        # 生成了一个 只含有next_symbol的（1,1）的张量\n","        # -1 表示在最后一个维度上进行拼接cat\n","        # 这行代码的作用是将next_symbol拼接到dec_input中，作为新一轮decoder的输入\n","        dec_input = torch.cat([dec_input.detach(), torch.tensor([[next_symbol]], dtype=enc_input.dtype).cuda()], -1) # dec_input: [1,当前词数]\n","        dec_outputs = model.Decoder(dec_input, enc_input, enc_outputs) # dec_outputs: [1, tgt_len, d_model]\n","        projected = model.generator(dec_outputs) # projected: [1, 当前生成的tgt_len, tgt_vocab_size]\n","        # max返回的是一个元组（最大值，最大值对应的索引），所以用[1]取到最大值对应的索引, 索引就是类别，即预测出的下一个词\n","        # keepdim为False会导致减少一维\n","        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1] # prob: [1],\n","        # prob是一个一维的列表，包含目前为止依次生成的词的索引，最后一个是新生成的（即下一个词的类别）\n","        # 因为注意力是依照前面的词算出来的，所以后生成的不会改变之前生成的\n","        next_symbol = prob.data[-1]\n","        if next_symbol == tgt_vocab['.']:\n","            flag = False\n","        print(next_symbol)\n","    return dec_input  # dec_input: [1,tgt_len]\n","\n","\n","# 测试\n","model = torch.load('MyTransformer.pth')\n","model.eval()\n","with torch.no_grad():\n","    # 手动从loader中取一个batch的数据\n","    enc_inputs, _, _ = next(iter(loader))\n","    enc_inputs = enc_inputs.cuda()\n","    for i in range(len(enc_inputs)):\n","        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab['S'])\n","        predict  = model(enc_inputs[i].view(1, -1), greedy_dec_input) # predict: [batch_size * tgt_len, tgt_vocab_size]\n","        predict = predict.data.max(dim=-1, keepdim=False)[1]\n","        '''greedy_dec_input是基于贪婪策略生成的，而贪婪解码的输出是基于当前时间步生成的假设的输出。这意味着它可能不是最优的输出，因为它仅考虑了每个时间步的最有可能的单词，而没有考虑全局上下文。\n","        因此，为了获得更好的性能评估，通常会将整个输入序列和之前的假设输出序列传递给模型，以考虑全局上下文并允许模型更准确地生成输出\n","        '''\n","        print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 探究一下多头注意力从(batch_size, seq_len, d_model) 到 (batch_size,n_heads, seq_len, d_k/v)的意义\n","\n","# 1、这是初始的q\n","q = torch.arange(120).reshape(2,5,12)\n","print(q)\n","print('------------------')\n","batch_size = 2\n","seq_len = 5\n","d_model = 12\n","n_heads = 3\n","d_k = 4\n","\n","# 2、分成n_heads个头\n","new_q = q.view(batch_size, -1, n_heads, d_k).transpose(1,2)\n","# 上面一行代码的形状变化：(2,5,12) -> (2,5,3,4) -> (2,3,5,4)\n","# 意义变化：最初是batch_size为2，一个batch中有2个句子，一个句子包含5个词，每个词由长度为12的向量表示\n","# 最后仍然是batch_size为2，但一个batch中有3个头，每个头包含一个句子，每个句子包含5个词，但每个词由长度为4的向量表示\n","\n","print(new_q)\n","print(new_q.shape) # torch.Size([2, 3, 5, 4])\n","print('------------------')\n","\n","# 3、将n_heads个头合并\n","final_q = q.transpose(1,2).contiguous().view(batch_size, -1, d_model)\n","print(final_q)\n","print(final_q.shape)\n","print('------------------')\n","\n","# 按原来的concat实现拼回去元素顺序和最初不同了，因此改成下面这种实现\n","final_q2 = torch.cat([new_q[:,i,:,:] for i in range(new_q.size(1))], dim=-1)\n","print(final_q2)\n","print(final_q2.shape)\n","\n","# \n"]}],"metadata":{"kernelspec":{"display_name":"HENN","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":2}
